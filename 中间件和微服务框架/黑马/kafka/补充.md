# 起步

- 导入依赖：原版本的`kafka-client`可能与kafka版本不对应而报错，可以排除该依赖另外导入其他版本的。

~~~xml
<parent>
    <groupId>org.springframework.boot</groupId>
    <artifactId>spring-boot-starter-parent</artifactId>
    <version>2.7.6</version>
</parent>

<dependencies>
    <dependency>
        <groupId>org.springframework.boot</groupId>
        <artifactId>spring-boot-starter-test</artifactId>
    </dependency>
    <dependency>
        <groupId>org.springframework.kafka</groupId>
        <artifactId>spring-kafka</artifactId>
        <exclusions>
            <exclusion>
                <groupId>org.apache.kafka</groupId>
                <artifactId>kafka-clients</artifactId>
            </exclusion>
        </exclusions>
    </dependency>
    <dependency>
        <groupId>org.apache.kafka</groupId>
        <artifactId>kafka-clients</artifactId>
        <version>3.1.2</version>
    </dependency>
</dependencies>
~~~
- 配置kafka的相关配置项
~~~yaml
logging:
  level:
    com.mq: debug
spring:
  kafka:
    bootstrap-servers: 111.173.105.157:9092
    producer:
      acks: all		# 设置生产者确认机制
      retries: 10	# 设置生产者的重试次数
      key-serializer: org.apache.kafka.common.serialization.StringSerializer
      value-serializer: org.apache.kafka.common.serialization.StringSerializer
    consumer:
      group-id: ${spring.application.name}-test
      key-deserializer: org.apache.kafka.common.serialization.StringDeserializer
      value-deserializer: org.apache.kafka.common.serialization.StringDeserializer
      enable-auto-commit: false
~~~

- 编写消费者

~~~java
@Component
public class SpringKafkaListener {
	@KafkaListener(topics = "test")
	public void testListener(String msg) throws InterruptedException {
		System.out.println(msg);
		Thread.sleep(500);
	}
}
~~~

- 编写生产者

~~~java
@SpringBootTest
public class SpringKafkaTest {
	@Resource
	private KafkaTemplate<String, String> kafkaTemplate;

	@Test
	public void testSend() {
		kafkaTemplate.send("test", "dd", "测试消息");
	}
}
~~~

# 消息丢失

## 生产者丢失

生产者发送消息到broker丢失的解决方法：

- 设置异步发送，发送失败使用回调进行记录或重发。

~~~java
public void testSend() throws InterruptedException {
    ListenableFuture<SendResult<String, String>> future = kafkaTemplate.send("test", "test1", "测试消息");
    future.addCallback(new ListenableFutureCallback<>() {
        @Override
        public void onFailure(Throwable ex) {
            log.error("消息发送失败：{}", ex.getMessage());
            //进行记录或重发
        }

        @Override
        public void onSuccess(SendResult<String, String> result) {
            RecordMetadata recordMetadata = result.getRecordMetadata();
            String topic = recordMetadata.topic();
            long offset = recordMetadata.offset();
            int partition = recordMetadata.partition();
            log.debug("消息发送成功，topic：{}, offset: {}, partition: {}", topic, offset, partition);
        }
    });
    Thread.sleep(2000);
}
~~~



- 失败重试，参数配置，可以设置重试次数。

	- yml中设置：`spring.kafka.producer.retries: 10`
	- 代码设置。

	~~~java
	@Bean
	public ProducerFactory<String, String> producerFactory(KafkaProperties properties) {
	    Map<String, Object> configProps = properties.buildProducerProperties();
	    // 可以在这里添加或覆盖其他属性  
	    return new DefaultKafkaProducerFactory<>(configProps);
	}
	~~~

## broker丢失

消息在broker中存储丢失，解决方法：

发送确认acks，选择all，让所有的副本都参与保存数据后的确认。（acks默认1，只需leader确认,0不需确认）

- yml中设置：`spring.kafka.producer.acks: all`

## 消费者丢失

消费者从broker接收信息丢失，解决方法：

- 关闭自动提交偏移量，开启手动提交偏移量。

	~~~java
	@Resource
	private ConsumerFactory<String, String> consumerFactory;
	
	@Bean
	public ConcurrentKafkaListenerContainerFactory<String, String> kafkaListenerContainerFactory() {
	    ConcurrentKafkaListenerContainerFactory<String, String> factory = new ConcurrentKafkaListenerContainerFactory<>();
	    factory.setConsumerFactory(consumerFactory);
	    // 设置手动确认模式
	    factory.getContainerProperties().setAckMode(ContainerProperties.AckMode.MANUAL);
	    return factory;
	}
	~~~

- 提交方式最后是：同步 + 异步提交。

	- `try{异步提交} catch{...} finally{同步提交}`
	- spring-kafka中无法直接配置同步

	~~~java
	@KafkaListener(topics = "test", groupId = "consumer-g1")
	public void testListener(String msg, Acknowledgment acknowledgment) throws InterruptedException {
	    try {
	        log.debug("接收到消息：" + msg);
	        // 接受消息成功，异步提交偏移量
	        acknowledgment.acknowledge();
	    } catch (Exception e) {
	        // 出现异常
	        log.error("消息消费失败：{}", e.getMessage());
	        // 异步等待5s后返回nack；如果返回了nack，则消息会立刻重发
	        acknowledgment.nack(Duration.ofSeconds(5));
	    }
	}
	~~~

- 消费者分区分配策略使用`stricky`粘性分配策略

~~~java
@Bean
public ConsumerFactory<String, String> producerFactory(KafkaProperties properties) {
    Map<String, Object> configProps = properties.buildProducerProperties();
    configProps.put(ConsumerConfig.PARTITION_ASSIGNMENT_STRATEGY_CONFIG, "sticky");
    return new DefaultKafkaConsumerFactory<>(configProps);
}
~~~

